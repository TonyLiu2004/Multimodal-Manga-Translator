{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e19bc8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tonyl\\Documents\\Multimodal-Manga-Translator\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoImageProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import re\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1f12f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path.cwd().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7e19e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 264/264 [00:00<00:00, 1236.16it/s, Materializing param=encoder.pooler.dense.weight]                                       \n",
      "The tied weights mapping and config for this model specifies to tie decoder.bert.embeddings.word_embeddings.weight to decoder.cls.predictions.decoder.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The tied weights mapping and config for this model specifies to tie decoder.cls.predictions.bias to decoder.cls.predictions.decoder.bias, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "\u001b[1mVisionEncoderDecoderModel LOAD REPORT\u001b[0m from: kha-white/manga-ocr-base\n",
      "Key                                  | Status     |  | \n",
      "-------------------------------------+------------+--+-\n",
      "decoder.bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "The image processor of type `ViTImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n",
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\tonyl\\\\Documents\\\\Multimodal-Manga-Translator\\\\backend\\\\models\\\\kha-white\\\\processor\\\\preprocessor_config.json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"kha-white/manga-ocr-base\")\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"kha-white/manga-ocr-base\")\n",
    "# processor = AutoImageProcessor.from_pretrained(\"kha-white/manga-ocr-base\")\n",
    "\n",
    "# tokenizer.save_pretrained(ROOT / \"backend\" / \"models\" / \"Kha-white\" / \"tokenizer\")\n",
    "# model.save_pretrained(ROOT / \"backend\" / \"models\" / \"Kha-white\" / \"model\")\n",
    "# processor.save_pretrained(ROOT / \"backend\" / \"models\" / \"Kha-white\" / \"processor\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ROOT / \"backend\" / \"models\" / \"Kha-white\" / \"tokenizer\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(ROOT / \"backend\" / \"models\" / \"Kha-white\" / \"model\")\n",
    "processor = AutoImageProcessor.from_pretrained(ROOT / \"backend\" / \"models\" / \"Kha-white\" / \"processor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "236315e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR Result: 明 日 は 忍 者 学 校 の 卒 業 試 験 だ ぞ ! ! お 前 は 前 回 も そ の 前 も 試 験 に 落 ち て る ! !\n"
     ]
    }
   ],
   "source": [
    "# 1. Prepare the image\n",
    "image_path = \"./bubble_test_results/text/im5.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "# 3. Generate the text\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "generated_ids = model.generate(\n",
    "    inputs.pixel_values, \n",
    "    max_new_tokens=64,\n",
    "    num_beams=5,           # Search for more likely sequences\n",
    "    do_sample=False,       # Keep it deterministic for OCR\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(f\"OCR Result: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16007a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
