{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d42f1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tonyl\\Documents\\Multimodal-Manga-Translator\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2900e426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_parameters` for 'rope_type'='dynamic': {'rope_theta', 'beta_slow', 'beta_fast', 'mscale', 'alpha', 'mscale_all_dim'}\n",
      "Unrecognized keys in `rope_parameters` for 'rope_type'='dynamic': {'rope_theta', 'beta_slow', 'beta_fast', 'mscale', 'alpha', 'mscale_all_dim'}\n",
      "Loading weights: 100%|██████████| 355/355 [00:00<00:00, 1000.52it/s, Materializing param=model.norm.weight]                               \n",
      "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"tencent/HY-MT1.5-1.8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\")  # You may want to use bfloat16 and/or move to GPU here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840f19d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:04<00:00,  4.51s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./TencentHY/tokenizer\\\\tokenizer_config.json',\n",
       " './TencentHY/tokenizer\\\\chat_template.jinja',\n",
       " './TencentHY/tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.save_pretrained(\"/backend/models/TencentHY/model\")\n",
    "# tokenizer.save_pretrained(\"/backend/models/TencentHY/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ca8d164",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_text = \"\"\"\n",
    "    加油！你可以的！\n",
    "\n",
    "    你是不是在吃醋？\n",
    "\n",
    "    给我个面子，别再说了。\n",
    "\n",
    "    你这个人真的很二。\n",
    "\n",
    "    简直是自寻死路！\n",
    "\n",
    "    你找死\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dce070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go! You can do it!\n",
      "\n",
      "Are you being jealous?\n",
      "\n",
      "Give me some respect and stop talking about it.\n",
      "\n",
      "You’re really something… kind of weird.\n",
      "\n",
      "It’s like inviting death into your own life!\n",
      "\n",
      "You’re seeking death.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a professional Manhua translator. Translate dialogue into natural, punchy English used in action manga. Use genre-appropriate slang (e.g., 'courting death', 'brat', 'senior'). Sometimes the words you see are incorrect, so make assumptions when necessary on what the sentence really means. You will output nothing but translations.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"Translate: {ocr_text}\"}\n",
    "]\n",
    "tokenized_chat = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True       \n",
    ").to(model.device)\n",
    "\n",
    "\n",
    "outputs = model.generate(\n",
    "    **tokenized_chat, \n",
    "    max_new_tokens=2048\n",
    ")\n",
    "\n",
    "prompt_length = tokenized_chat.input_ids.shape[1]\n",
    "new_tokens = outputs[0][prompt_length:]\n",
    "\n",
    "output_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1d8237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
